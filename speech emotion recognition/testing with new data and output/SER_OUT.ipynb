{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e977203d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted Emotion: ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\SER\\ðŸ˜‚\\ðŸ˜‚ (7).wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46d876b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 688ms/step\n",
      "Predicted Emotion: ðŸ˜”\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\SER\\ðŸ˜”\\ðŸ˜” (33).wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56311238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted Emotion: ðŸ˜¡\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\SER\\ðŸ˜¡\\ðŸ˜¡ (50).wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22662b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Emotion: ðŸ˜±\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\SER\\ðŸ˜±\\ðŸ˜± (310).wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a065607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 607ms/step\n",
      "Predicted Emotion: ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\Downloads\\female.wav.wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a85edf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 717ms/step\n",
      "Predicted Emotion: ðŸ˜±\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\Angry.wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5987028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Emotion: ðŸ˜¡\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\Downloads\\ma.wav.wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72be2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Emotion: ðŸ˜±\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\Downloads\\Senorita-Shawn-Mendes(musicdownload.cc).mp3\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c630fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 636ms/step\n",
      "Predicted Emotion: ðŸ˜±\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('combined_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\Downloads\\Travis-Scott---FEiN(trendymusic.in).mp3\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34d957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Emotion: ðŸ˜±\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\Angry.wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cebdc3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Emotion: ðŸ˜”\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\record_out (1).wav\"\n",
    "\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf669c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Emotion: ðŸ˜¡\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('ser_model.keras')\n",
    "\n",
    "# Function to extract MFCC features from audio\n",
    "def extract_mfcc(wave, sr, n_mfcc=13, hop_length=512, n_fft=2048, max_pad_len=174):\n",
    "    mfccs = librosa.feature.mfcc(y=wave, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "    # Truncate or pad the sequences to have a consistent length\n",
    "    if mfccs.shape[1] > max_pad_len:\n",
    "        mfccs = mfccs[:, :max_pad_len]\n",
    "    else:\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    return mfccs\n",
    "\n",
    "# Function to load and preprocess a single audio file\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfccs = extract_mfcc(wave, sr)\n",
    "    return mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1])  # Reshape for model input\n",
    "\n",
    "# Path to the input audio file for testing\n",
    "input_audio_path = r\"C:\\Users\\Anish\\OneDrive\\Desktop\\eng_m1.wav\"\n",
    "# Load and preprocess the input audio file\n",
    "input_features = load_and_preprocess_audio(input_audio_path)\n",
    "\n",
    "# Perform prediction using the loaded model\n",
    "predictions = loaded_model.predict([input_features, input_features])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "emotion_labels = ['ðŸ˜‚', 'ðŸ˜”', 'ðŸ˜¡', 'ðŸ˜±']\n",
    "predicted_label = emotion_labels[np.argmax(predictions)]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a89cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
